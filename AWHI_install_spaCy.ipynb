{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Testing spaCy to build skills for text analysis \n",
    "   - spaCy: Open-source library for industrial-strength Natural Language Processing\n",
    "       - helps you build applications that process and \"understand\" large volumes of text \n",
    "   - THink: spaCy's next generation machine learning library for deep learning with text \n",
    "   - prodigy: a radically eficient data collection and annotation tool. powered by active learning \n",
    "   - DataStore: coming soon: pre-trained, customisable models for a variety of languages and domains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ this didn't run properly on my mac laptop\n",
    "## Installation instructions \n",
    "- following installation instructions found here: https://spacy.io/usage\n",
    "- jupyter notebook shortcuts for mac: https://gist.github.com/kidpixo/f4318f8c8143adee5b40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/2a/057bca697905031a6179227278a49fe9e7841a6d444bc2e13f1266b9f7dc/spacy-2.1.8-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (34.4MB)\n",
      "\u001b[K     |████████████████████████████████| 34.4MB 281kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/11/37da628920bf2999bd8c4ffc40908413622486d5dbc4e60d87a58c428367/cymem-2.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (52kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in /Users/curryt/anaconda3/lib/python3.7/site-packages (from spacy) (1.16.4)\n",
      "Collecting preshed<2.1.0,>=2.0.1 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/fe/2f2e8c91541785f2abe0d51f37eb00356513b9ff3d24fb27fd5b59e18264/preshed-2.0.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (145kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 11.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/41/9e934e2b8a2cdae447ed1923a94f98c2d70c898b65af6635f5fe55f7ed4d/blis-0.2.4-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 26.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<7.1.0,>=7.0.8 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/36/a9b47b517266b8ece0c6aa5477deb9c71b809645e8f92871a2480c1a8407/thinc-7.0.8-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 51.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=0.0.6 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/f9/092f2f006150c92b13096811f5797f2430f55cb2756cb83ee421f59b2a3f/srsly-0.0.7-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (271kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 9.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /Users/curryt/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/bd/faace403086ee922afc74e5615cb8c21020fcf5d5667314e943c08f71fde/murmurhash-1.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /Users/curryt/anaconda3/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.32.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/curryt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/curryt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/curryt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/curryt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Installing collected packages: cymem, preshed, wasabi, blis, srsly, murmurhash, plac, thinc, spacy\n",
      "Successfully installed blis-0.2.4 cymem-2.0.2 murmurhash-1.0.2 plac-0.9.6 preshed-2.0.1 spacy-2.1.8 srsly-0.0.7 thinc-7.0.8 wasabi-0.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^should see \"successfully installed blis-0.2.4\"\n",
    "- there are multiple ways to install this software, U just used the first option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- verifying to see if all models are comparable with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Installed models (spaCy v2.1.8) ======================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/Users/curryt/anaconda3/lib/python3.7/site-packages/spacy\u001b[0m\n",
      "\n",
      "No models found in your current environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ I currently don't have any models intsalled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1MB 11.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/g9/d9fj242n3kl9sj0_86nyrnz9x5n5t4/T/pip-ephem-wheel-cache-66y4bu19/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# download best-matching version of specific model for your spaCy installation\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ \"you can now load the model via spacy.load('en_core_web_sm')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models\n",
    "- downloadable statistical models for spaCy to predict linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#use spacy.load() to load model\n",
    "\n",
    "\n",
    "#example test \n",
    "doc = nlp(u\"This is a sentence.\")\n",
    "print([(w.text, w.pos_)for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example test \n",
    "\n",
    "#nlp = spacy.load(\"en\") <- this shortcut link doesn't work for me \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if you want to learn more about spaCy watch this video: https://www.youtube.com/watch?v=sqDHBH9IjRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models & languages\n",
    "- link: https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing test utilities via spaCy's requirments.txt </>\n",
    "# Our libraries\n",
    "#! cymem>=2.0.2,<2.1.0\n",
    "#! preshed>=2.0.1,<2.1.0\n",
    "#! thinc>=7.0.8,<7.1.0\n",
    "#! blis>=0.2.2,<0.3.0\n",
    "#! murmurhash>=0.28.0,<1.1.0\n",
    "#! wasabi>=0.2.0,<1.1.0\n",
    "#! srsly>=0.0.6,<1.1.0\n",
    "# Third party dependencies\n",
    "#! numpy>=1.15.0\n",
    "#! requests>=2.13.0,<3.0.0\n",
    "#! plac<1.0.0,>=0.9.6\n",
    "#! pathlib==1.0.1; python_version < \"3.4\"\n",
    "# Optional dependencies\n",
    "#! jsonschema>=2.6.0,<3.1.0\n",
    "# Development dependencies\n",
    "#! cython>=0.25\n",
    "#! pytest>=4.0.0,<4.1.0\n",
    "#! pytest-timeout>=1.3.0,<2.0.0\n",
    "#! mock>=2.0.0,<3.0.0\n",
    "#! flake8>=3.5.0,<3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -c \"import os; import spacy; print(os.path.dirname(spacy.__file__))\"\n",
    "#! pip install -r path/to/requirements.txt\n",
    "#! python -m pytest [spacy directory]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- couldn't get run test to work, I don't think I had the files downloaded \n",
    "- There's a spaCy 101: https://spacy.io/usage/spacy-101\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy 101 \n",
    "## Chapter 1: finding words, phrases, names and concepts \n",
    "\n",
    "- Introduction to spaCy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "#1. introduction to spaCy \n",
    "\n",
    "#import the English language class\n",
    "#processing pipeline\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#Create the nlp\n",
    "#includes language-specific rules for tokenization etc. \n",
    "nlp = English()\n",
    "\n",
    "#Created by processing a string of text with the nlp object \n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "#interate over tokens in a Doc \n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "#the token object \n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "#index into the doc to get a single token \n",
    "token = doc[1]\n",
    "\n",
    "#get the token text via the .text attribute \n",
    "print(token.text)\n",
    "\n",
    "#Q:\"world\" is doc[1] and not \"Hello\", why???\n",
    "#A: \"Hello\" is doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "#the span object \n",
    "\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "#A slice from the Doc is a Span object\n",
    "span = doc[1:4]\n",
    "\n",
    "#Get the span text via the .text attribute \n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:   [0, 1, 2, 3, 4]\n",
      "Text:    ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "#lexical attributes \n",
    "\n",
    "doc = nlp(\"It costs $5.\")\n",
    "\n",
    "#this will show the index # for each object\n",
    "print('Index:  ', [token.i for token in doc])\n",
    "\n",
    "#this will list all objects in doc \n",
    "print('Text:   ', [token.text for token in doc])\n",
    "\n",
    "#logical statement, true for anything that's a word or in the alphabet\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "\n",
    "#logical statment, true for  anything that's punctuation\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "\n",
    "#logical statement, true for anything that's a number\n",
    "print('like_num:', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Statistical models \n",
    "    - what are statistical models? \n",
    "        - enable spaCy to predict linguistic attributes in context \n",
    "            - part-of-speech tags\n",
    "            - syntactic dependencies \n",
    "            - named entities\n",
    "        - trained on labeled example texts \n",
    "        - can be updated with more examples to fine-tune predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#5.statistical model\n",
    "\n",
    "# can use pre-trained model packages using spacy download\n",
    "#\"en_core_web_sm\" package small English model, supports all core capabilities and is trained on web text\n",
    "! python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "import spacy \n",
    "\n",
    "# spacy dot loaf methof loads a model package by name and returns an nlp object\n",
    "# pacjage provides the binary weights that enable spaCy to make predictions\n",
    "#includes vocabulary, meta informatoin (language, pipeline)\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "#predicting part-of-speech tags\n",
    "\n",
    "#load the small english model \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "#iterate over the tokens\n",
    "for token in doc:\n",
    "    #print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)\n",
    "    \n",
    "    #attributes that return strings usually end with an underscore-attrubutes without the underscore return an ID\n",
    "    # the model orrectly predicted \"ate\" as a verb and \"pizza\" as a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "#predicting syntactic dependencies \n",
    "\n",
    "#we can also predict how the word's are related. like subjcet or object \n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    #\"dep underscore\" attribute returens the predicted dependency label\n",
    "    #head attribute returns the syntactic head token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "#predicting named entities\n",
    "\n",
    "#named entities are \"real world objects\" that are assigned a name ex: a person, an organization, a country\n",
    "\n",
    "#process a text\n",
    "#the doc dot ents property lets you access the named entities predicted by the model\n",
    "# returns an iterator od Span objects\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "#Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    #print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "    # \"label underscore\" attribute can print the entity tez and the entity label\n",
    "    # this example correctly predicting \"Apple\" as an organization, \"U.K.\" as a geoplitical entity and \"$1 billion\" as money \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "noun, proper singular\n",
      "direct object\n"
     ]
    }
   ],
   "source": [
    "#explain method \n",
    "# to get def for the most common tags and labels use spacy.explain\n",
    "\n",
    "a = spacy.explain('GPE')\n",
    "\n",
    "b = spacy.explain('NNP')\n",
    "\n",
    "c = spacy.explain('dobj')\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "#predicting named entities in context \n",
    "#models are statistical and not always right \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"New iPhone X release date leaed as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "#process the text \n",
    "doc = nlp(\"New iPhone X release date leaed as Apple reveals pre-orders by mistake\")\n",
    "\n",
    "#iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and label\n",
    "    print(ent.text,ent.label_)\n",
    "    \n",
    "# get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text \n",
    "print(\"missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rule-based matching\n",
    "    - use spaCy's matcher, which lets you write rules ro find words and phrases in text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Why not just regular expressions? \n",
    "        - match on Doc objects, not just strings \n",
    "        - match on tokens and tooken attributes \n",
    "        - use the model's predictions \n",
    "        - example: \"duck\" (verb) vs. \"duck\" (noun)\n",
    "    \n",
    "    - match patterns\n",
    "        - match patterns are lists of dictionaries. each dictionary describes one token\n",
    "        - keys are the names of token attributes, mapped on their expected values \n",
    "        - we can write patterns using attributes predicted by the model\n",
    "        - ex matching token with the lemma \"buy\", plus a noun\n",
    "        - lemma is the base form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "#import the matcher \n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "#load a model and create the nlp object \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#add the pattern to the matcher \n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT':'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "#process some text \n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "#Call the matcher on the doc \n",
    "matches = matcher(doc )\n",
    "\n",
    "#Call the matcher on the doc \n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "#iterate over the matches \n",
    "for match_id, start, end in matches: \n",
    "    # Get the matched span \n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    \n",
    "#match_id: hash value of the pattern name \n",
    "#start: start index of matched span \n",
    "#end: end index of matched span "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
