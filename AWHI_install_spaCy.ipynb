{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Testing spaCy to build skills for text analysis \n",
    "   - spaCy: Open-source library for industrial-strength Natural Language Processing\n",
    "       - helps you build applications that process and \"understand\" large volumes of text \n",
    "   - THink: spaCy's next generation machine learning library for deep learning with text \n",
    "   - prodigy: a radically eficient data collection and annotation tool. powered by active learning \n",
    "   - DataStore: coming soon: pre-trained, customisable models for a variety of languages and domains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ this didn't run properly on my mac laptop\n",
    "## Installation instructions \n",
    "- following installation instructions found here: https://spacy.io/usage\n",
    "- jupyter notebook shortcuts for mac: https://gist.github.com/kidpixo/f4318f8c8143adee5b40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^should see \"successfully installed blis-0.2.4\"\n",
    "- there are multiple ways to install this software, U just used the first option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- verifying to see if all models are comparable with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ I currently don't have any models intsalled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download best-matching version of specific model for your spaCy installation\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ \"you can now load the model via spacy.load('en_core_web_sm')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models\n",
    "- downloadable statistical models for spaCy to predict linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#use spacy.load() to load model\n",
    "\n",
    "\n",
    "#example test \n",
    "doc = nlp(u\"This is a sentence.\")\n",
    "print([(w.text, w.pos_)for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example test \n",
    "\n",
    "#nlp = spacy.load(\"en\") <- this shortcut link doesn't work for me \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if you want to learn more about spaCy watch this video: https://www.youtube.com/watch?v=sqDHBH9IjRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models & languages\n",
    "- link: https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing test utilities via spaCy's requirments.txt </>\n",
    "# Our libraries\n",
    "#! cymem>=2.0.2,<2.1.0\n",
    "#! preshed>=2.0.1,<2.1.0\n",
    "#! thinc>=7.0.8,<7.1.0\n",
    "#! blis>=0.2.2,<0.3.0\n",
    "#! murmurhash>=0.28.0,<1.1.0\n",
    "#! wasabi>=0.2.0,<1.1.0\n",
    "#! srsly>=0.0.6,<1.1.0\n",
    "# Third party dependencies\n",
    "#! numpy>=1.15.0\n",
    "#! requests>=2.13.0,<3.0.0\n",
    "#! plac<1.0.0,>=0.9.6\n",
    "#! pathlib==1.0.1; python_version < \"3.4\"\n",
    "# Optional dependencies\n",
    "#! jsonschema>=2.6.0,<3.1.0\n",
    "# Development dependencies\n",
    "#! cython>=0.25\n",
    "#! pytest>=4.0.0,<4.1.0\n",
    "#! pytest-timeout>=1.3.0,<2.0.0\n",
    "#! mock>=2.0.0,<3.0.0\n",
    "#! flake8>=3.5.0,<3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -c \"import os; import spacy; print(os.path.dirname(spacy.__file__))\"\n",
    "#! pip install -r path/to/requirements.txt\n",
    "#! python -m pytest [spacy directory]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- couldn't get run test to work, I don't think I had the files downloaded \n",
    "- There's a spaCy 101: https://spacy.io/usage/spacy-101\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy 101 \n",
    "## Chapter 1: finding words, phrases, names and concepts \n",
    "\n",
    "- Introduction to spaCy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "#1. introduction to spaCy \n",
    "\n",
    "#import the English language class\n",
    "#processing pipeline\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#Create the nlp\n",
    "#includes language-specific rules for tokenization etc. \n",
    "nlp = English()\n",
    "\n",
    "#Created by processing a string of text with the nlp object \n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "#interate over tokens in a Doc \n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "#the token object \n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "#index into the doc to get a single token \n",
    "token = doc[1]\n",
    "\n",
    "#get the token text via the .text attribute \n",
    "print(token.text)\n",
    "\n",
    "#Q:\"world\" is doc[1] and not \"Hello\", why???\n",
    "#A: \"Hello\" is doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "#the span object \n",
    "\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "#A slice from the Doc is a Span object\n",
    "span = doc[1:4]\n",
    "\n",
    "#Get the span text via the .text attribute \n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:   [0, 1, 2, 3, 4]\n",
      "Text:    ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "#lexical attributes \n",
    "\n",
    "doc = nlp(\"It costs $5.\")\n",
    "\n",
    "#this will show the index # for each object\n",
    "print('Index:  ', [token.i for token in doc])\n",
    "\n",
    "#this will list all objects in doc \n",
    "print('Text:   ', [token.text for token in doc])\n",
    "\n",
    "#logical statement, true for anything that's a word or in the alphabet\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "\n",
    "#logical statment, true for  anything that's punctuation\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "\n",
    "#logical statement, true for anything that's a number\n",
    "print('like_num:', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Statistical models \n",
    "    - what are statistical models? \n",
    "        - enable spaCy to predict linguistic attributes in context \n",
    "            - part-of-speech tags\n",
    "            - syntactic dependencies \n",
    "            - named entities\n",
    "        - trained on labeled example texts \n",
    "        - can be updated with more examples to fine-tune predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#5.statistical model\n",
    "\n",
    "# can use pre-trained model packages using spacy download\n",
    "#\"en_core_web_sm\" package small English model, supports all core capabilities and is trained on web text\n",
    "! python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "import spacy \n",
    "\n",
    "# spacy dot loaf methof loads a model package by name and returns an nlp object\n",
    "# pacjage provides the binary weights that enable spaCy to make predictions\n",
    "#includes vocabulary, meta informatoin (language, pipeline)\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "#predicting part-of-speech tags\n",
    "\n",
    "#load the small english model \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "#iterate over the tokens\n",
    "for token in doc:\n",
    "    #print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)\n",
    "    \n",
    "    #attributes that return strings usually end with an underscore-attrubutes without the underscore return an ID\n",
    "    # the model orrectly predicted \"ate\" as a verb and \"pizza\" as a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "#predicting syntactic dependencies \n",
    "\n",
    "#we can also predict how the word's are related. like subjcet or object \n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "    #\"dep underscore\" attribute returens the predicted dependency label\n",
    "    #head attribute returns the syntactic head token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "#predicting named entities\n",
    "\n",
    "#named entities are \"real world objects\" that are assigned a name ex: a person, an organization, a country\n",
    "\n",
    "#process a text\n",
    "#the doc dot ents property lets you access the named entities predicted by the model\n",
    "# returns an iterator od Span objects\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "#Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    #print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "    # \"label underscore\" attribute can print the entity tez and the entity label\n",
    "    # this example correctly predicting \"Apple\" as an organization, \"U.K.\" as a geoplitical entity and \"$1 billion\" as money \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "noun, proper singular\n",
      "direct object\n"
     ]
    }
   ],
   "source": [
    "#explain method \n",
    "# to get def for the most common tags and labels use spacy.explain\n",
    "\n",
    "a = spacy.explain('GPE')\n",
    "\n",
    "b = spacy.explain('NNP')\n",
    "\n",
    "c = spacy.explain('dobj')\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "#predicting named entities in context \n",
    "#models are statistical and not always right \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"New iPhone X release date leaed as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "#process the text \n",
    "doc = nlp(\"New iPhone X release date leaed as Apple reveals pre-orders by mistake\")\n",
    "\n",
    "#iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and label\n",
    "    print(ent.text,ent.label_)\n",
    "    \n",
    "# get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text \n",
    "print(\"missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rule-based matching\n",
    "    - use spaCy's matcher, which lets you write rules ro find words and phrases in text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Why not just regular expressions? \n",
    "        - match on Doc objects, not just strings \n",
    "        - match on tokens and tooken attributes \n",
    "        - use the model's predictions \n",
    "        - example: \"duck\" (verb) vs. \"duck\" (noun)\n",
    "    \n",
    "    - match patterns\n",
    "        - match patterns are lists of dictionaries. each dictionary describes one token\n",
    "        - keys are the names of token attributes, mapped on their expected values \n",
    "        - we can write patterns using attributes predicted by the model\n",
    "        - ex matching token with the lemma \"buy\", plus a noun\n",
    "        - lemma is the base form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "#import the matcher \n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "#load a model and create the nlp object \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#add the pattern to the matcher \n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT':'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "#process some text \n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "#Call the matcher on the doc \n",
    "matches = matcher(doc )\n",
    "\n",
    "#Call the matcher on the doc \n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "#iterate over the matches \n",
    "for match_id, start, end in matches: \n",
    "    # Get the matched span \n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    \n",
    "#match_id: hash value of the pattern name \n",
    "#start: start index of matched span \n",
    "#end: end index of matched span "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
