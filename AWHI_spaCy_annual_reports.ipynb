{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spaCy on annual report files \n",
    "- testing spaCy to do text analysis \n",
    "- starting with a single file, then write code to include all files from annual report google shared folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math as m \n",
    "import os , sys\n",
    "import spacy\n",
    "from collections import Counter \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opening files test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open/read a text file , no output \n",
    "annual_report_1963 = open(\"annualreporto18491961smit_djvu.txt\",\"r\")\n",
    "\n",
    "#prints entire file in output\n",
    "#[:0000] used to determine how many words to display \n",
    "print(annual_report_1963.read()[:1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^I had issues with reading my text file \n",
    "- found my directory using os (import os) \n",
    "- next two steps helps you find your directory and files in your current directory\n",
    "- link: https://stackoverflow.com/questions/12201928/python-open-gives-ioerror-errno-2-no-such-file-or-directory\n",
    "- built-in functions in python: https://docs.python.org/3/library/functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to list files in current working directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check what directory you're in \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency test\n",
    "\n",
    "- using methods and examples from soaCy for beginners - NLP\n",
    "- link: https://blog.ekbana.com/nlp-for-beninners-using-spacy-6161cf48a229\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example from website \n",
    "#7. Get word frequnecy \n",
    "#testing to see if it runs \n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#remove stopwords and punctuations\n",
    "\n",
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "word_freq = Counter(words)\n",
    "\n",
    "common_words = word_freq.most_common(5)\n",
    "\n",
    "print (common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- link: https://spacy.io/api/token\n",
    "    - shows the use and extensions for different containers, pipelines and functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# most common word count using spaCy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example from website\n",
    "#modified with Mike 8/11/19\n",
    "\n",
    "#need to test with a file saved on your own computer, so the information in open() will change\n",
    "#this information will be different for each computer in open()\n",
    "\n",
    "#open the file you want to do word fequency\n",
    "#with open function will close files once the function is done running\n",
    "#directory is imported from github in the same directory so no need to include /Desktop/User\n",
    "with open(\"annualreporto18491961smit_djvu.txt\",\"r\") as annual_report_1963:\n",
    "\n",
    "    #need to add \".read()\" so that it will look at the text in the open file\n",
    "    doc = nlp(annual_report_1963.read())\n",
    "\n",
    "#testing the results of \n",
    "#results using text ext. funciton\n",
    "#useful to make words token to cut white space len(), stop code token.is_stop , and punction token.is_punct \n",
    "words_text = [token.text for token in doc if token.is_stop != True and token.is_punct != True and len(token.text.strip()) > 0]\n",
    "\n",
    "word_text_freq = Counter(words_text)\n",
    "\n",
    "common_words_text = word_text_freq.most_common(50)\n",
    "print(\"Common words by text function\")\n",
    "print(common_words_text)\n",
    "\n",
    "#results using norm_ ext. function\n",
    "#grouping variation of words i.e (dog, DOG,Dog, dogs, dog's)\n",
    "#everything is searchable by lowercase\n",
    "words_norm = [token.norm_ for token in doc if token.is_stop != True and token.is_punct != True and len(token.text.strip()) > 0]\n",
    "\n",
    "word_norm_freq = Counter(words_norm)\n",
    "\n",
    "common_words_norm = word_norm_freq.most_common(50)\n",
    "print(\"Common words by norm function\")\n",
    "print(common_words_norm)\n",
    "\n",
    "#results using lemma_ ext. function \n",
    "#grouping variation of words and tense on verbs i.e (see, saw, seen), (dog, DOG,Dog, dogs, dog's)\n",
    "#everything is searchable by lowercase \n",
    "words_lemma = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True and len(token.text.strip()) > 0]\n",
    "\n",
    "word_lemma_freq = Counter(words_lemma)\n",
    "\n",
    "common_words_lemma = word_lemma_freq.most_common(50)\n",
    "print(\"Common words by lemma function\")\n",
    "print(common_words_lemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word count specific word given using spaCy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imput word you want to count, not counted with spacy token so doesn't count variations\n",
    "#only counts words like \"Smithsonian\" as it's typed\n",
    "#input which word to count using text word \n",
    "word_freq['Smithsonian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemma_freq['smithsonian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_norm_freq['smithsonian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ websites reference: https://github.com/explosion/spaCy/issues/1851\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word count using io and spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to do the same thing in a different way\n",
    "#this is an older way of writing this code\n",
    "#example with io\n",
    "\n",
    "import io\n",
    "\n",
    "ff = io.open(\"annualreporto18491961smit_djvu.txt\",\"r\", encoding='utf-8')\n",
    "doc = nlp(ff.read())\n",
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "\n",
    "word_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = word_freq.most_common(5)\n",
    "\n",
    "print (common_words)\n",
    "\n",
    "ff.close()\n",
    "\n",
    "#this gives the same results as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ websites reference: https://github.com/explosion/spaCy/issues/1851"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing word counts without installed package  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word count with no packages test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#word count for \"Smithsonian\" annual report 1963\n",
    "counts = 0\n",
    "with open (\"annualreporto18491961smit_djvu.txt\") as openfile:\n",
    "    for line in openfile:\n",
    "        if \"Smithsonian\" in line:\n",
    "        #for part in line.split():\n",
    "            #if \"Smithsonian\" in part: \n",
    "            counts += 1 \n",
    "           # print(line)\n",
    "        if \"SMITHSONIAN\" in line:\n",
    "            counts += 1\n",
    "    #else:\n",
    "        #print(\"not found\")\n",
    "    print(\"word count = \",counts)\n",
    "# I need to write code so that Smithsonian and SMITHSONIAN and smithsonian match    \n",
    "              \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^website reference: https://stackoverflow.com/questions/38101704/how-to-count-items-boolean-after-if-statements-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening multiple files and counting words test\n",
    "## code will not run properly, missing inputs\n",
    "- website reference:https://stackoverflow.com/questions/16997165/unique-word-frequency-in-multiple-files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will not run properly without change input, just a sample from a website\n",
    "from collections import Counter\n",
    "from glob import iglob\n",
    "import re\n",
    "import os\n",
    "\n",
    "def remove_garbage(text):\n",
    "    \"\"\"Replace non-word (non-alphanumeric) chars in text with spaces,\n",
    "       then convert and return a lowercase version of the result.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "topwords = 100\n",
    "folderpath = 'path/to/directory'\n",
    "counter = Counter()\n",
    "for filepath in iglob(os.path.join(folderpath, '*.txt')):\n",
    "    with open(filepath) as file:\n",
    "        counter.update(remove_garbage(file.read()).split())\n",
    "\n",
    "for word, count in counter.most_common(topwords):\n",
    "    print('{}: {}'.format(count, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# web searches \n",
    "- libraries in python: https://www.edureka.co/blog/python-libraries/\n",
    "- best NLP libraries for python: https://elitedatascience.com/python-nlp-libraries\n",
    "- def functions: https://www.codementor.io/kaushikpal/user-defined-functions-in-python-8s7wyc8k2\n",
    "- opening multiple files using context manager: https://stackoverflow.com/questions/21680473/how-can-i-open-multiple-files-number-of-files-unknown-beforehand-using-with-o\n",
    "- counting word frequency with python: https://programminghistorian.org/en/lessons/counting-frequencies\n",
    "- functions in spacy: https://spacy.io/api/top-level\n",
    "- using spacy examples: https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/\n",
    "- using unicodedata: https://github.com/LightTable/Python/issues/24\n",
    "- text classifications in spacy: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "- read,open,write,add to files: https://www.geeksforgeeks.org/reading-writing-text-files-python/\n",
    "- using io: https://github.com/explosion/spaCy/issues/1851\n",
    "- using io: https://stackoverflow.com/questions/37530891/python-pandas-nameerror-stringio-is-not-defined\n",
    "- how to count/ boolean: https://stackoverflow.com/questions/38101704/how-to-count-items-boolean-after-if-statements-python\n",
    "- opening files: https://codereview.stackexchange.com/questions/31395/python-open-multiple-files\n",
    "- opening files: https://stackoverflow.com/questions/38991923/how-to-open-multiple-files-in-a-directory/38992988\n",
    "- using countermanager: https://stackoverflow.com/questions/21680473/how-can-i-open-multiple-files-number-of-files-unknown-beforehand-using-with-o\n",
    "- what is contextlib: https://docs.python.org/3/library/contextlib.html\n",
    "- word frequency with multiple files: https://stackoverflow.com/questions/16997165/unique-word-frequency-in-multiple-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count on multiple files \n",
    "- Using annual reports\n",
    "- With spaCy, os, and pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 count for token women 9\n",
      "2 count for token women 0\n",
      "3 count for token women 0\n",
      "1 count for token women 0\n",
      "2 count for token women 0\n",
      "3 count for token women 0\n",
      "1 count for token women 2\n",
      "2 count for token women 0\n",
      "3 count for token women 0\n"
     ]
    }
   ],
   "source": [
    "# code for word count on annual reports with token:\"women\" \n",
    "\n",
    "n = [1,2,3]\n",
    "reports = os.listdir(\"Annual Report Data\")                                           #creating a list of reports in the Annual Report Data folder \n",
    "nlp.max_length = 5000000\n",
    "\n",
    "for annual_report in reports:                                                                     #so that it will only run the first 3 files \n",
    "    with open(os.path.join(\"Annual Report Data\", annual_report)) as report_path: #opening one file at a time in a loop using with open()\n",
    "        for i in n:     \n",
    "            doc = nlp(report_path.read())\n",
    "            \n",
    "            words_lemma = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True and len(token.text.strip()) > 0]\n",
    "\n",
    "            word_lemma_freq = Counter(words_lemma)\n",
    "\n",
    "            women_word_count = word_lemma_freq['women']\n",
    "\n",
    "            print(i , \"count for token women\", women_word_count)\n",
    "            #else:\n",
    "                #print(\"finished\")\n",
    "                #if i >= 3:\n",
    "                    #print (\"complete\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^explaining common error\n",
    "- Error: \"ValueError: [E088] Text of length 1764531 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\"\n",
    "    - will attempt to increase the nlp.max_length limit or split files\n",
    "        - nlp max length is 100,000 characters \n",
    "    - reference link increase max len: https://github.com/explosion/spaCy/issues/2817\n",
    "        - increase as long as you don't run out of RAM: https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit\n",
    "    - link max len split files: https://stackoverflow.com/questions/48143769/spacy-nlp-library-what-is-maximum-reasonable-document-size\n",
    "    - total character count link: https://pythonexamples.org/python-count-number-of-characters-in-text-file/\n",
    "    - character count per character: https://www.w3resource.com/python-exercises/basic/python-basic-1-exercise-7.php\n",
    "    \n",
    "- Error: dead kernel will restart automatically \n",
    "    - saved notebooks, shutdown, CTRL + C in terminal and relaunch in terminal. But my still get the dead kernel pop up when running the code above. \n",
    "    - Error: results were weird with lemma code in with open loop and for i in n, first 3 iterations said 9 for women word count, but then continued to run for i in n and output 0 for women word count. \n",
    "    - relaunched without i in n and the kernal dead "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### character count per character \n",
    "- finding length of file, to increase nlp.max_length()\n",
    "- checking this annual report file: annualreporto18491961smit_djvu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pprint\n",
    "file_input = input('File Name:')\n",
    "\n",
    "with open(file_input, 'r') as info:\n",
    "    count = collections.Counter(info.read().upper())\n",
    "    value = pprint.pformat(count)\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total character count including spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open file in read mode\n",
    "file = open(\"annualreporto18491961smit_djvu.txt\", \"r\")\n",
    "\n",
    "#read the content of file\n",
    "data = file.read()\n",
    "\n",
    "#get the length of the data\n",
    "number_of_characters = len(data)\n",
    "\n",
    "print('Number of characters in text file :', number_of_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"Annual Report Data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
